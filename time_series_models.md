# A Comparative Analysis of Models for Time Series Forecasting 



| **Model Family** | **Pros for Time Series Forecasting** | **Cons for Time Series Forecasting** |
|------------------|--------------------------------------|--------------------------------------|
| **ARIMA** | - Highly interpretable and simple to understand.<br>- Effective for short-term forecasts with clear, linear patterns.<br>- Well-established statistical foundation. | - Assumes linear relationships and struggles with complex, non-linear data.<br>- Requires the time series to be stationary, often needing manual differencing.<br>- Struggles to capture long-term dependencies. |
| **Support Vector Regression (SVR)** | - Can model complex, non-linear relationships using the kernel trick.<br>- Good generalization performance and robust against overfitting, especially with many features.<br>- Does not assume a specific distribution for the data. | - Computationally expensive and memory-intensive; scales poorly with large datasets (long time series).<br>- Highly sensitive to the choice of kernel and hyperparameters, requiring extensive tuning.<br>- Can perform poorly on noisy data. |
| **Tree-Based Ensembles (Random Forest, Gradient Boosting)** | - Excellent at capturing complex, non-linear interactions and patterns within the data.<br>- Robust to outliers and do not require feature scaling.<br>- Can handle a large number of features (including external variables).<br>- Gradient Boosting often achieves state-of-the-art accuracy. | - Cannot extrapolate trends (predictions are bounded by training range).<br>- Requires preprocessing (e.g., de-trending) to handle trends.<br>- Models are "black boxes" and less interpretable than linear models.<br>- Gradient Boosting is prone to overfitting and sensitive to hyperparameters. |
| **Neural Networks (MLP-based like N-HiTS)** | - State-of-the-art accuracy on long-horizon forecasting tasks.<br>- Computationally efficient and fast, with smaller memory footprint than Transformers.<br>- Can model complex patterns, multiple seasonalities, and non-stationary data.<br>- Can incorporate past observations, future known inputs, and static variables. | - Complex "black box" architecture makes interpretation difficult.<br>- Performance highly depends on data context.<br>- Can be data-hungry and prone to overfitting if not properly configured. |
| **Recurrent Neural Networks (RNN/LSTM)** | - Specifically designed for sequential data; inherently understands temporal order.<br>- LSTMs excel at capturing long-term dependencies and patterns.<br>- Can model complex non-linearities and handle non-stationary data. | - Sequential processing makes training slow and non-parallelizable.<br>- Simple RNNs suffer from vanishing/exploding gradient problems.<br>- Computationally expensive and require large datasets. |
| **Convolutional Neural Networks (CNN)** | - Training is highly parallelizable and efficient compared to RNNs.<br>- Excellent at extracting local patterns from sequences (e.g., motifs in sensor data).<br>- Less susceptible to vanishing/exploding gradient issues. | - Limited by kernel size and struggles to capture long-range dependencies.<br>- Less intuitive for time series since they lack a memory mechanism. |
| **Transformer Models** | - Superior ability to capture very long-range dependencies using self-attention.<br>- Highly parallelizable architecture enables faster training on long sequences.<br>- Easily incorporates multiple time series and external variables. | - Self-attention has quadratic computational/memory complexity â€” expensive for long series.<br>- Requires large datasets to train effectively and prevent overfitting.<br>- Complex architecture that can be difficult to design and tune. |
